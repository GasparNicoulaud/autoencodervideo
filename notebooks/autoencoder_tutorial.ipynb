{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Autoencoder Tutorial\n",
    "\n",
    "This notebook demonstrates how to use the video autoencoder with programmatic latent space control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Video, display\n",
    "\n",
    "from src.models import VideoAutoencoder, load_pretrained_model\n",
    "from src.latent import LatentManipulator, interpolate_latents\n",
    "from src.utils import load_video, save_video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize or load model\n",
    "model = VideoAutoencoder(latent_dim=512, base_channels=64)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Or load pretrained\n",
    "# model = load_pretrained_model('vae_ucf101', device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Latent Space Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random samples\n",
    "z_random = torch.randn(4, 512).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_videos = model.decode(z_random)\n",
    "\n",
    "print(f\"Generated shape: {generated_videos.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Programmatic Weight Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manipulator = LatentManipulator(model)\n",
    "manipulator.save_original_params()\n",
    "\n",
    "# Example 1: Scale weights\n",
    "manipulator.scale_weights('decoder_fc', scale_factor=1.5)\n",
    "\n",
    "# Example 2: Add controlled noise\n",
    "manipulator.add_noise('encoder_fc', noise_std=0.1)\n",
    "\n",
    "# Get statistics\n",
    "stats = manipulator.compute_weight_statistics()\n",
    "for layer, layer_stats in stats.items():\n",
    "    print(f\"\\n{layer}:\")\n",
    "    for param_type, param_stats in layer_stats.items():\n",
    "        print(f\"  {param_type}: mean={param_stats['mean']:.4f}, std={param_stats['std']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Latent Space Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two random latent codes\n",
    "z1 = torch.randn(1, 512).to(device)\n",
    "z2 = torch.randn(1, 512).to(device)\n",
    "\n",
    "# Interpolate between them\n",
    "interpolated = interpolate_latents(z1[0], z2[0], steps=10)\n",
    "\n",
    "# Decode interpolations\n",
    "interpolated_videos = []\n",
    "with torch.no_grad():\n",
    "    for z in interpolated:\n",
    "        video = model.decode(z.unsqueeze(0))\n",
    "        interpolated_videos.append(video)\n",
    "\n",
    "print(f\"Created {len(interpolated_videos)} interpolated videos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Custom Latent Manipulation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_custom_transform(z, transform_type='sine'):\n",
    "    \"\"\"Apply custom transformations to latent codes\"\"\"\n",
    "    if transform_type == 'sine':\n",
    "        # Apply sinusoidal modulation\n",
    "        t = torch.linspace(0, 2*np.pi, z.shape[1]).to(z.device)\n",
    "        return z * torch.sin(t)\n",
    "    \n",
    "    elif transform_type == 'threshold':\n",
    "        # Threshold activations\n",
    "        return torch.where(torch.abs(z) > 1.0, z, torch.zeros_like(z))\n",
    "    \n",
    "    elif transform_type == 'amplify_dims':\n",
    "        # Amplify specific dimensions\n",
    "        z_new = z.clone()\n",
    "        z_new[:, :100] *= 2.0  # Amplify first 100 dimensions\n",
    "        z_new[:, -100:] *= 0.5  # Reduce last 100 dimensions\n",
    "        return z_new\n",
    "    \n",
    "    return z\n",
    "\n",
    "# Test transforms\n",
    "z_test = torch.randn(1, 512).to(device)\n",
    "\n",
    "for transform in ['sine', 'threshold', 'amplify_dims']:\n",
    "    z_transformed = apply_custom_transform(z_test, transform)\n",
    "    with torch.no_grad():\n",
    "        video = model.decode(z_transformed)\n",
    "    print(f\"Applied {transform} transform\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interactive Latent Space Explorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, FloatSlider, IntSlider\n",
    "\n",
    "def explore_latent_dimension(dim=0, value=0.0):\n",
    "    \"\"\"Interactive widget to explore individual latent dimensions\"\"\"\n",
    "    z = torch.zeros(1, 512).to(device)\n",
    "    z[0, dim] = value\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        video = model.decode(z)\n",
    "        # Display first frame\n",
    "        frame = video[0, :, 0].cpu().numpy().transpose(1, 2, 0)\n",
    "        frame = (frame + 1) / 2  # Denormalize\n",
    "        \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(frame)\n",
    "    plt.title(f'Dimension {dim} = {value:.2f}')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Create interactive widget\n",
    "interact(explore_latent_dimension,\n",
    "         dim=IntSlider(min=0, max=511, step=1, value=0),\n",
    "         value=FloatSlider(min=-3.0, max=3.0, step=0.1, value=0.0));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Batch Processing with Custom Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch of modifications\n",
    "modifications = [\n",
    "    {'type': 'scale', 'factor': 0.5},\n",
    "    {'type': 'scale', 'factor': 1.0},\n",
    "    {'type': 'scale', 'factor': 1.5},\n",
    "    {'type': 'scale', 'factor': 2.0},\n",
    "]\n",
    "\n",
    "z_base = torch.randn(1, 512).to(device)\n",
    "results = []\n",
    "\n",
    "for mod in modifications:\n",
    "    manipulator.restore_original_params()\n",
    "    \n",
    "    if mod['type'] == 'scale':\n",
    "        manipulator.scale_weights('decoder_fc', scale_factor=mod['factor'])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        video = model.decode(z_base)\n",
    "        results.append(video)\n",
    "    \n",
    "    print(f\"Applied {mod['type']} with factor {mod['factor']}\")\n",
    "\n",
    "# Reset to original\n",
    "manipulator.restore_original_params()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}